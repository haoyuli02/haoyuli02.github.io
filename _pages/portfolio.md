---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

### Interpretability through Training Samples: Data Attribution for Diffusion Models
Tong Xie\*, Haoyu Li\*, Andrew Bai, Cho-jui Hsieh

(Under Review)

Data attribution methods help interpret how neural networks behave by linking the model behavior to their training data. We extend the first-order influence approximation, TracIn, to diffusion models by incorporating the denoising timestep dynamics. We demonstrate that this influence estimation may be biased due to dominating gradient norms. To this end, Diffusion-ReTrac with a renormalization technique is introduced, enabling notably more localized influence estimation and the targeted attribution of training samples.

<figure>
  <img src="https://github.com/txie1/txie1.github.io/assets/117710195/c7977afc-5273-4da5-8469-900a90ce9af8" alt="image_tracing-1" width="500"/>
  <figcaption>Image-tracing on outlier model.</figcaption>

  <img scr="https://github.com/txie1/txie1.github.io/assets/117710195/2977d254-0c11-4c13-912b-10d5b53005f9" alt="acc" width="300"/>
</figure>

